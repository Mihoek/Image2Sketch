# -*- coding: utf-8 -*-
"""computer_vis3.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WuPK7How-WsdoPfUby3SXfuAqfdFTVDL
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import tensorflow as tf
from glob import glob
from tqdm import tqdm
import urllib.request
from zipfile import ZipFile
import shutil
from google.colab import drive
import gc

# Pour le d√©bogage
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Monter Google Drive
def mount_drive():
    """
    Monte Google Drive pour acc√©der aux donn√©es
    """
    try:
        drive.mount('/content/drive')
        logger.info("Google Drive mont√© avec succ√®s.")
        return True
    except Exception as e:
        logger.error(f"Erreur lors du montage de Google Drive: {e}")
        return False

from google.colab import drive
drive.mount('/content/drive')

# D√©finir les chemins
BASE_DIR = "/content"
DRIVE_BASE_DIR = "/content/drive/MyDrive/Project_computer_vision/dataset/edge2real"  # Modifier selon votre structure
DATA_DIR = os.path.join(BASE_DIR, "data")

# Chemins pour les images
INPUT_DIR = os.path.join(DATA_DIR, "images_combinees")
OUTPUT_REAL = os.path.join(DATA_DIR, "images_reelles")
OUTPUT_SKETCH = os.path.join(DATA_DIR, "images_sketch")

print("="*50)
print("D√âMARRAGE DU PROJET DE TRANSFORMATION D'IMAGE EN CROQUIS")
print("="*50 + "\n")

def create_directories():
    """
    Cr√©e les r√©pertoires n√©cessaires pour le projet
    """
    dirs = [
        DATA_DIR,
        os.path.join(DATA_DIR, 'original'),
        os.path.join(DATA_DIR, 'sketches'),
        os.path.join(DATA_DIR, 'processed'),
        os.path.join(BASE_DIR, 'models'),
        INPUT_DIR,
        OUTPUT_REAL,
        OUTPUT_SKETCH
    ]

    for split in ["train", "test", "val"]:
        dirs.extend([
            os.path.join(INPUT_DIR, split),
            os.path.join(OUTPUT_REAL, split),
            os.path.join(OUTPUT_SKETCH, split)
        ])

    for directory in dirs:
        os.makedirs(directory, exist_ok=True)

    logger.info("Tous les r√©pertoires requis ont √©t√© cr√©√©s avec succ√®s.")

# √âtape 1: Configuration initiale
print("[1/8] üõ†Ô∏è Configuration initiale...")
create_directories()
print("‚úÖ R√©pertoires cr√©√©s avec succ√®s\n")

def copy_data_from_drive():
    """
    Copie les donn√©es depuis Google Drive vers le r√©pertoire de travail
    """
    if not os.path.exists(DRIVE_BASE_DIR):
        logger.error(f"Le dossier {DRIVE_BASE_DIR} n'existe pas sur Google Drive.")
        return False

    try:
        # Parcourir les sous-dossiers train, test, val
        for split in ["train", "test", "val"]:
            src_dir = os.path.join(DRIVE_BASE_DIR, split)
            dst_dir = os.path.join(INPUT_DIR, split)

            if os.path.exists(src_dir):
                # V√©rifier si des fichiers sont pr√©sents
                files = glob(os.path.join(src_dir, "*.jpg")) + glob(os.path.join(src_dir, "*.png"))

                if files:
                    logger.info(f"Copie des fichiers depuis {src_dir} vers {dst_dir}...")

                    # Cr√©er le r√©pertoire de destination s'il n'existe pas
                    os.makedirs(dst_dir, exist_ok=True)

                    # Copier les fichiers
                    for file in tqdm(files):
                        shutil.copy2(file, dst_dir)

                    logger.info(f"{len(files)} fichiers copi√©s avec succ√®s pour le split {split}.")
                else:
                    logger.warning(f"Aucun fichier trouv√© dans {src_dir}.")
            else:
                logger.warning(f"Le dossier {src_dir} n'existe pas sur Google Drive.")

        return True
    except Exception as e:
        logger.error(f"Erreur lors de la copie des donn√©es: {e}")
        return False

# √âtape 2: Connexion √† Google Drive
print("[2/8] üìÅ Connexion √† Google Drive...")
drive_mounted = mount_drive()
if not drive_mounted:
   print("‚ùå √âchec du montage de Google Drive")
   exit(1)
print("‚úÖ Google Drive mont√© avec succ√®s\n")

# √âtape 3: Copie des donn√©es
print("[3/8] üîÑ Copie des donn√©es depuis Google Drive...")
if not copy_data_from_drive():
  print("‚ö†Ô∏è Aucune donn√©e copi√©e ou erreur lors de la copie")
else:
  print("‚úÖ Donn√©es copi√©es avec succ√®s\n")

def process_combined_images():
    """
    Traite toutes les images combin√©es pour tous les splits (train, test, val)
    Version optimis√©e pour la m√©moire
    """
    for split in ["train", "test", "val"]:
        input_split_dir = os.path.join(INPUT_DIR, split)
        output_real_dir = os.path.join(OUTPUT_REAL, split)
        output_sketch_dir = os.path.join(OUTPUT_SKETCH, split)

        os.makedirs(output_real_dir, exist_ok=True)
        os.makedirs(output_sketch_dir, exist_ok=True)

        image_paths = glob(os.path.join(input_split_dir, "*.jpg")) + glob(os.path.join(input_split_dir, "*.png"))

        if not image_paths:
            logger.warning(f"Aucune image trouv√©e dans {input_split_dir}")
            continue

        logger.info(f"{len(image_paths)} images trouv√©es dans {split}/")

        # Traitement par lots pour √©conomiser la m√©moire
        batch_size = 100  # Ajuster selon la RAM disponible
        for i in tqdm(range(0, len(image_paths), batch_size), desc=f"Traitement {split}"):
            batch_paths = image_paths[i:i+batch_size]

            for path in batch_paths:
                try:
                    # Lecture de l'image avec v√©rification de m√©moire
                    img = cv2.imread(path)
                    if img is None:
                        logger.warning(f"Image non lue : {path}")
                        continue

                    # Diviser l'image
                    h, w = img.shape[:2]
                    img_real = img[:, :w//2]
                    img_sketch = img[:, w//2:]

                    if img_real.shape != img_sketch.shape:
                        logger.warning(f"Dimensions incompatibles pour {path}")
                        continue

                    # Enregistrement imm√©diat pour lib√©rer la m√©moire
                    filename = os.path.basename(path)
                    cv2.imwrite(os.path.join(output_real_dir, "real_" + filename), img_real)
                    cv2.imwrite(os.path.join(output_sketch_dir, "sketch_" + filename), img_sketch)

                except Exception as e:
                    logger.error(f"Erreur avec {path}: {e}")

                # Lib√©ration explicite de la m√©moire
                del img, img_real, img_sketch

            # Nettoyage interm√©diaire
            gc.collect()

        logger.info(f"Split {split} trait√© avec succ√®s.")

# Classe pour le g√©n√©rateur de donn√©es
class DataGeneratorHelper:
    """
    Classe d'assistance pour interfacer les g√©n√©rateurs avec l'API Keras
    """
    def __init__(self, split, batch_size=32, img_size=(256, 256)):
        self.split = split
        self.batch_size = batch_size
        self.img_size = img_size
        self.shape = None
        self.input_shape = None
        self.output_shape = None

        # D√©finir les chemins de donn√©es
        self.real_paths = []
        self.sketch_paths = []

        if split:
            real_images = sorted(glob(os.path.join(OUTPUT_REAL, split, "*.jpg")) +
                          glob(os.path.join(OUTPUT_REAL, split, "*.png")))
            sketch_images = sorted(glob(os.path.join(OUTPUT_SKETCH, split, "*.jpg")) +
                            glob(os.path.join(OUTPUT_SKETCH, split, "*.png")))

            if len(real_images) > 0 and len(sketch_images) > 0:
                self.real_paths = real_images
                self.sketch_paths = sketch_images

                # D√©finir les formes
                self.input_shape = (img_size[0], img_size[1], 3)
                self.output_shape = (img_size[0], img_size[1], 1)
                self.shape = (self.input_shape, self.output_shape)

    def get_shape(self):
        """Renvoie la forme des donn√©es (X_shape, Y_shape)"""
        return self.shape

    def get_length(self):
        """Renvoie le nombre d'√©chantillons"""
        return len(self.real_paths)

    def __iter__(self):
        """Retourne un it√©rateur sur les donn√©es"""
        num_samples = self.get_length()

        for start_idx in range(0, num_samples, self.batch_size):
            end_idx = min(start_idx + self.batch_size, num_samples)

            X_batch, Y_batch = [], []

            for i in range(start_idx, end_idx):
                try:
                    # Charger l'image r√©elle
                    real_img = cv2.imread(self.real_paths[i])
                    real_img = cv2.cvtColor(real_img, cv2.COLOR_BGR2RGB)
                    real_img = cv2.resize(real_img, self.img_size)

                    # Charger l'image sketch
                    sketch_img = cv2.imread(self.sketch_paths[i], cv2.IMREAD_GRAYSCALE)
                    sketch_img = cv2.resize(sketch_img, self.img_size)

                    # Normaliser les images √† [0, 1]
                    real_img = real_img / 255.0
                    sketch_img = sketch_img / 255.0

                    # Ajouter une dimension pour le canal de gris
                    sketch_img = np.expand_dims(sketch_img, axis=-1)

                    X_batch.append(real_img)
                    Y_batch.append(sketch_img)

                except Exception as e:
                    logger.error(f"Erreur lors du pr√©traitement de {self.real_paths[i]}: {e}")

            if X_batch and Y_batch:
                yield np.array(X_batch), np.array(Y_batch)

# √âtape 4: Traitement des images
    print("[4/8] üñºÔ∏è Traitement des images combin√©es...")
    process_combined_images()
    print("‚úÖ Images trait√©es avec succ√®s\n")

def preprocess_data(img_size=(256, 256)):
    """
    Pr√©traite les images pour l'entra√Ænement du mod√®le

    Args:
        img_size: Taille cible pour les images

    Returns:
        Un tuple (train_data, val_data, test_data) contenant les g√©n√©rateurs de donn√©es
    """
    train_data = DataGeneratorHelper("train", batch_size=32, img_size=img_size)
    val_data = DataGeneratorHelper("val", batch_size=32, img_size=img_size)
    test_data = DataGeneratorHelper("test", batch_size=32, img_size=img_size)

    logger.info(f"Donn√©es d'entra√Ænement: {train_data.get_length()} images")
    logger.info(f"Donn√©es de validation: {val_data.get_length()} images")
    logger.info(f"Donn√©es de test: {test_data.get_length()} images")

    return train_data, val_data, test_data

def build_unet_model(input_size=(256, 256, 3)):
    """
    Construit un mod√®le U-Net pour la transformation d'image en croquis

    Args:
        input_size: Forme des images d'entr√©e (hauteur, largeur, canaux)

    Returns:
        model: Mod√®le U-Net compil√©
    """
    inputs = Input(input_size)

    # Encoder (chemin de down-sampling)
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    # Bridge
    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)
    drop4 = Dropout(0.5)(conv4)

    # Decoder (chemin de up-sampling)
    up5 = concatenate([UpSampling2D(size=(2, 2))(drop4), conv3], axis=3)
    conv5 = Conv2D(256, 3, activation='relu', padding='same')(up5)
    conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)

    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv2], axis=3)
    conv6 = Conv2D(128, 3, activation='relu', padding='same')(up6)
    conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)

    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv1], axis=3)
    conv7 = Conv2D(64, 3, activation='relu', padding='same')(up7)
    conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)

    # Couche de sortie
    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)

    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])

    return model

# √âtape 5: Pr√©paration des donn√©es
print("[5/8] üß† Pr√©paration des donn√©es pour l'entra√Ænement...")
train_data, val_data, test_data = preprocess_data(img_size=(256, 256))

if train_data.get_length() == 0:
   print("‚ùå Aucune donn√©e d'entra√Ænement disponible")
   exit(1)
else:
   print(f"‚úÖ Donn√©es pr√©par√©es: {train_data.get_length()} √©chantillons d'entra√Ænement\n")

# Mise √† jour de la fonction d'entra√Ænement pour utiliser les g√©n√©rateurs
def train_model_with_generators(train_data, val_data, epochs=20):
    """
    Entra√Æne le mod√®le U-Net avec des g√©n√©rateurs de donn√©es pour √©conomiser la m√©moire

    Args:
        train_data: G√©n√©rateur pour les donn√©es d'entra√Ænement
        val_data: G√©n√©rateur pour les donn√©es de validation
        epochs: Nombre d'√©poques d'entra√Ænement

    Returns:
        history: Historique d'entra√Ænement
        model: Mod√®le entra√Æn√©
    """
    # V√©rifier si les donn√©es sont disponibles
    if train_data.get_length() == 0:
        logger.error("Aucune donn√©e d'entra√Ænement disponible")
        return None, None

    # Obtenir la forme des donn√©es
    input_shape, _ = train_data.get_shape()

    # Construire le mod√®le
    model = build_unet_model(input_size=input_shape)

    # Cr√©ation des callbacks
    model_checkpoint = ModelCheckpoint(
        os.path.join(BASE_DIR, 'models', 'image_to_sketch_model.h5'),
        monitor='val_loss',
        save_best_only=True,
        mode='min'
    )

    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )

    # Cr√©er des listes pour stocker l'historique manuellement
    history_dict = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}

    # Entra√Ænement manuel pour un meilleur contr√¥le de la m√©moire
    for epoch in range(epochs):
        logger.info(f"Epoch {epoch+1}/{epochs}")

        # M√©triques d'entra√Ænement
        train_loss = []
        train_acc = []

        # Entra√Ænement
        for X_batch, Y_batch in train_data:
            history = model.train_on_batch(X_batch, Y_batch)
            train_loss.append(history[0])
            train_acc.append(history[1])

        # Calculer les moyennes
        epoch_loss = np.mean(train_loss)
        epoch_acc = np.mean(train_acc)

        history_dict['loss'].append(epoch_loss)
        history_dict['accuracy'].append(epoch_acc)

        # Validation si les donn√©es sont disponibles
        val_loss = []
        val_acc = []

        if val_data.get_length() > 0:
            for X_val, Y_val in val_data:
                val_history = model.test_on_batch(X_val, Y_val)
                val_loss.append(val_history[0])
                val_acc.append(val_history[1])

            epoch_val_loss = np.mean(val_loss)
            epoch_val_acc = np.mean(val_acc)

            history_dict['val_loss'].append(epoch_val_loss)
            history_dict['val_accuracy'].append(epoch_val_acc)

            logger.info(f"loss: {epoch_loss:.4f} - accuracy: {epoch_acc:.4f} - val_loss: {epoch_val_loss:.4f} - val_accuracy: {epoch_val_acc:.4f}")

            # V√©rifier early stopping
            if len(history_dict['val_loss']) > early_stopping.patience:
                if np.argmin(history_dict['val_loss']) < epoch - early_stopping.patience:
                    logger.info("Early stopping")
                    break
        else:
            history_dict['val_loss'].append(0)
            history_dict['val_accuracy'].append(0)
            logger.info(f"loss: {epoch_loss:.4f} - accuracy: {epoch_acc:.4f}")

        # Sauvegarder le meilleur mod√®le
        if val_data.get_length() > 0:
            if len(history_dict['val_loss']) == 1 or history_dict['val_loss'][-1] < min(history_dict['val_loss'][:-1]):
                model.save(os.path.join(BASE_DIR, 'models', 'image_to_sketch_model.h5'))
                logger.info("Mod√®le sauvegard√©")
        else:
            if epoch % 5 == 0:  # Sauvegarde tous les 5 epochs sans validation
                model.save(os.path.join(BASE_DIR, 'models', 'image_to_sketch_model.h5'))

        # Nettoyage de la m√©moire
        gc.collect()

    # Sauvegarder le mod√®le final
    model.save(os.path.join(BASE_DIR, 'models', 'image_to_sketch_final_model.h5'))

    # Cr√©er un objet history similaire √† celui de Keras
    class TrainingHistory:
        def __init__(self, history_dict):
            self.history = history_dict

    return TrainingHistory(history_dict), model

# √âtape 6: Entra√Ænement du mod√®le
print("[6/8] üèãÔ∏è Entra√Ænement du mod√®le U-Net...")
history, model = train_model_with_generators(train_data, val_data, epochs=50)
if history is None:
    print("‚ùå √âchec de l'entra√Ænement du mod√®le")
    exit(1)
else:
    print("‚úÖ Mod√®le entra√Æn√© avec succ√®s\n")

# prompt: save model to drive

def save_model_to_drive(model, drive_path):
    """
    Sauvegarde un mod√®le Keras sur Google Drive.

    Args:
        model: Le mod√®le Keras √† sauvegarder.
        drive_path: Le chemin complet sur Google Drive o√π sauvegarder le mod√®le
                    (par exemple, '/content/drive/MyDrive/Colab Notebooks/my_model.h5').
    """
    try:
        # Assurez-vous que le r√©pertoire existe sur Drive
        drive_dir = os.path.dirname(drive_path)
        if not os.path.exists(drive_dir):
            os.makedirs(drive_dir, exist_ok=True)
            logger.info(f"R√©pertoire cr√©√© sur Drive: {drive_dir}")

        # Sauvegarder le mod√®le
        model.save(drive_path)
        logger.info(f"Mod√®le sauvegard√© sur Google Drive √†: {drive_path}")
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde du mod√®le sur Drive: {e}")

# √âtape 7: Sauvegarde du mod√®le sur Google Drive
print("[7/8] üíæ Sauvegarde du mod√®le sur Google Drive...")
# D√©finissez le chemin de sauvegarde sur votre Google Drive
MODEL_SAVE_PATH_DRIVE = os.path.join(DRIVE_BASE_DIR, 'image_to_sketch_model_saved.h5')

# V√©rifiez si le mod√®le a √©t√© entra√Æn√© avec succ√®s
if model is not None:
    save_model_to_drive(model, MODEL_SAVE_PATH_DRIVE)
    print(f"‚úÖ Mod√®le sauvegard√© sur Drive √†: {MODEL_SAVE_PATH_DRIVE}\n")
else:
    print("‚ùå Impossible de sauvegarder le mod√®le car il n'a pas √©t√© entra√Æn√©\n")

# √âtape 8: √âvaluation du mod√®le (Optionnel)
# Vous pouvez ajouter ici le code pour √©valuer le mod√®le sur l'ensemble de test
# using model.evaluate(test_data)
print("[8/8] üéâ Fin du script.")
print("="*50 + "\n")

def evaluate_model(model, test_data):
    """
    √âvalue le mod√®le sur les donn√©es de test

    Args:
        model: Mod√®le entra√Æn√©
        test_data: G√©n√©rateur pour les donn√©es de test

    Returns:
        evaluation: M√©triques d'√©valuation (loss, accuracy)
    """
    if test_data.get_length() == 0:
        logger.warning("Aucune donn√©e de test disponible")
        return None, None

    losses = []
    accuracies = []

    for X_test, Y_test in test_data:
        evaluation = model.evaluate(X_test, Y_test, verbose=0)
        losses.append(evaluation[0])
        accuracies.append(evaluation[1])

    avg_loss = np.mean(losses)
    avg_accuracy = np.mean(accuracies)

    logger.info(f"Test Loss: {avg_loss}")
    logger.info(f"Test Accuracy: {avg_accuracy}")

    return avg_loss, avg_accuracy

def visualize_results(model, test_data, num_samples=5):
    """
    Visualise les pr√©dictions du mod√®le

    Args:
        model: Mod√®le entra√Æn√©
        test_data: G√©n√©rateur pour les donn√©es de test
        num_samples: Nombre d'√©chantillons √† visualiser
    """
    if test_data.get_length() == 0:
        logger.warning("Aucune donn√©e de test disponible pour la visualisation")
        return

    # R√©cup√©rer quelques √©chantillons pour la visualisation
    try:
        test_generator = iter(test_data)
        X_samples, Y_samples = next(test_generator)

        # Limiter au nombre d'√©chantillons demand√©s
        X_test = X_samples[:num_samples]
        Y_test = Y_samples[:num_samples]

        # G√©n√©rer les pr√©dictions
        Y_pred = model.predict(X_test)

        plt.figure(figsize=(15, 5*num_samples))

        for i in range(min(num_samples, len(X_test))):
            # Image originale
            plt.subplot(num_samples, 3, i*3 + 1)
            plt.title('Image Originale')
            plt.imshow(X_test[i])
            plt.axis('off')

            # Vrai croquis
            plt.subplot(num_samples, 3, i*3 + 2)
            plt.title('Vrai Croquis')
            plt.imshow(Y_test[i].reshape(Y_test[i].shape[0], Y_test[i].shape[1]), cmap='gray')
            plt.axis('off')

            # Croquis pr√©dit
            plt.subplot(num_samples, 3, i*3 + 3)
            plt.title('Croquis Pr√©dit')
            plt.imshow(Y_pred[i].reshape(Y_pred[i].shape[0], Y_pred[i].shape[1]), cmap='gray')
            plt.axis('off')

        plt.tight_layout()
        plt.savefig(os.path.join(BASE_DIR, 'model_predictions.png'))
        plt.show()

    except StopIteration:
        logger.warning("Pas assez d'√©chantillons disponibles pour la visualisation")
    except Exception as e:
        logger.error(f"Erreur lors de la visualisation: {e}")

def plot_training_history(history):
    """
    Trace l'historique d'entra√Ænement

    Args:
        history: Historique d'entra√Ænement
    """
    plt.figure(figsize=(15, 5))

    # Loss
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig(os.path.join(BASE_DIR, 'training_history.png'))
    plt.show()
    logger.info("Historique d'entra√Ænement trac√© avec succ√®s.")

# √âtape 7: √âvaluation du mod√®le
print("[7/8] üìä √âvaluation du mod√®le...")
if test_data.get_length() > 0:
    evaluate_model(model, test_data)
    plot_training_history(history)
    visualize_results(model, test_data, num_samples=5)
    print("‚úÖ √âvaluation termin√©e\n")
else:
    print("‚ö†Ô∏è Aucune donn√©e de test disponible pour l'√©valuation\n")